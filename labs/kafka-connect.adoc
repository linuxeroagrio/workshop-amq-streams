=== Kafka Connect

We will start with the production-ready Kafka cluster without security:

----
cat << EOF | oc replace -f -
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: production-ready
  namespace: amq-streams
spec:
  kafka:
    replicas: 3
    listeners:
    - name: plain
      port: 9092
      type: internal
      tls: false
    - name: tls
      port: 9093
      type: internal
      tls: true
    config:
      offsets.topic.replication.factor: 3
      transaction.state.log.replication.factor: 3
      transaction.state.log.min.isr: 2
    storage:
      type: persistent-claim
      size: 3Gi
      deleteClaim: false
  zookeeper:
    replicas: 3
    storage:
      type: persistent-claim
      size: 1Gi
      deleteClaim: false
  entityOperator:
    topicOperator: {}
    userOperator: {}
EOF
----

Next We will deploy a KafkaConnect cluster.

----
cat << EOF | oc apply -f -
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaConnect
metadata:
  name: connect-cluster
  namespace: amq-streams
  annotations:
    strimzi.io/use-connector-resources: "true"
spec:
  image: quay.io/linuxeroagrio/custom-kafka-connect:latest
  replicas: 1
  bootstrapServers: production-ready-kafka-bootstrap.amq-streams.svc:9093
  tls:
    trustedCertificates:
    - secretName: production-ready-cluster-ca-cert
      certificate: ca.crt
  config:
    group.id: connect-cluster
    offset.storage.topic: connect-cluster-offsets
    config.storage.topic: connect-cluster-configs
    status.storage.topic: connect-cluster-status
    config.storage.replication.factor: -1
    offset.storage.replication.factor: -1
    status.storage.replication.factor: -1
EOF
----

Watch for the creation of the KafkaConnect cluster.

We will deploy a file source that reads the contents of the file and put that on the `lines` topic.

----
cat << EOF | oc apply -f -
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaConnector
metadata:
  name: local-file-source
  namespace: amq-streams
  labels:
    strimzi.io/cluster: connect-cluster
spec:
  class: org.apache.kafka.connect.file.FileStreamSourceConnector
  tasksMax: 1
  config: 
    file: "/tmp/test.source.txt" 
    topic: lines
EOF
----

Now the job `local-file-source` is configured to ingest data to the `lines` topic in our cluster.

Let's check that the job works.
Since the job will ingest data from a local file to lines topic, we need to log into the KafkaConnect cluster pod and write some messages

----
CONNECT_POD_NAME=$(oc get pods -l app.kubernetes.io/instance=connect-cluster -n amq-streams -o jsonpath='{.items[0].metadata.name}')

oc -n amq-streams rsh ${CONNECT_POD_NAME}

while true; do echo "Date from file $(date)" >> /tmp/test.source.txt; sleep 3; done
----

Open a new Terminal and log into bastion, validate that the messages are recieved to the topic

----
oc -n amq-streams exec production-ready-kafka-0 -- bin/kafka-console-consumer.sh --bootstrap-server production-ready-kafka-bootstrap:9092 --topic lines --from-beginning
----

The output should be the dates emitted by the while in the KafkaConnector:

----
{"schema":{"type":"string","optional":false},"payload":"Date from file Mon Mar 18 09:13:33 UTC 2024"}
{"schema":{"type":"string","optional":false},"payload":"Date from file Mon Mar 18 09:13:36 UTC 2024"}
{"schema":{"type":"string","optional":false},"payload":"Date from file Mon Mar 18 09:13:39 UTC 2024"}
{"schema":{"type":"string","optional":false},"payload":"Date from file Mon Mar 18 09:13:42 UTC 2024"}
{"schema":{"type":"string","optional":false},"payload":"Date from file Mon Mar 18 09:13:45 UTC 2024"}
{"schema":{"type":"string","optional":false},"payload":"Date from file Mon Mar 18 09:13:48 UTC 2024"}
{"schema":{"type":"string","optional":false},"payload":"Date from file Mon Mar 18 09:13:51 UTC 2024"}
{"schema":{"type":"string","optional":false},"payload":"Date from file Mon Mar 18 09:13:54 UTC 2024"}
{"schema":{"type":"string","optional":false},"payload":"Date from file Mon Mar 18 09:13:57 UTC 2024"}
----

Press CTRL+C in both terminals.