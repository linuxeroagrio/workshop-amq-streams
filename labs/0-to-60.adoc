== AMQ Streams on OpenShift from 0 to 60

In this module you will learn how to install AMQ Streams on OpenShift.

=== AMQ Streams installation files

All the components necessary for installing AMQ Streams have been downloaded and extracted locally.
Once you log into the workstation machine, you should find a directory named `kafka`.
Let's navigate to the folder.

----
cd kafka
----

This directory contains three items:

* the unextracted zip file named `install_and_examples.zip`
* the contents of the zip file, as two folders: `install` and `examples`

[NOTE]
.Where does the zip file come from?
The installation files for AMQ Streams can be downloaded from link:https://access.redhat.com/jbossnetwork/restricted/listSoftware.html?downloadType=distributions&product=jboss.amq.streams[this location].
You can find this information in the reference documentation https://access.redhat.com/documentation/en-us/red_hat_amq_streams/2.6/html/deploying_and_managing_amq_streams_on_openshift/deploy-tasks-prereqs_str#downloads-str[here].
The machines provisioned for the lab have downloaded and extracted this fine already.

Verify that this is the case by listing the contents of the directory.
Now we can start the lab.

=== Creating a new project for running the cluster operator

Log in as the administrator with the password supplied by the instructor.

    oc login -u system:admin

Create a project named `amq-streams`.

    oc new-project amq-streams

=== Configuring the cluster operator and installing it

The configuration files for the cluster operator are available in the `install/cluster-operator` folder.
Let's take a quick look.

----
$ ls install/cluster-operator
010-ServiceAccount-strimzi-cluster-operator.yaml
020-ClusterRole-strimzi-cluster-operator-role.yaml
020-RoleBinding-strimzi-cluster-operator.yaml
021-ClusterRoleBinding-strimzi-cluster-operator.yaml
021-ClusterRole-strimzi-cluster-operator-role.yaml
022-ClusterRole-strimzi-cluster-operator-role.yaml
022-RoleBinding-strimzi-cluster-operator.yaml
023-ClusterRole-strimzi-cluster-operator-role.yaml
023-RoleBinding-strimzi-cluster-operator.yaml
030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml
030-ClusterRole-strimzi-kafka-broker.yaml
031-ClusterRole-strimzi-entity-operator.yaml
031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml
033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml
033-ClusterRole-strimzi-kafka-client.yaml
040-Crd-kafka.yaml
041-Crd-kafkaconnect.yaml
042-Crd-strimzipodset.yaml
043-Crd-kafkatopic.yaml
044-Crd-kafkauser.yaml
045-Crd-kafkamirrormaker.yaml
046-Crd-kafkabridge.yaml
047-Crd-kafkaconnector.yaml
048-Crd-kafkamirrormaker2.yaml
049-Crd-kafkarebalance.yaml
04A-Crd-kafkanodepool.yaml
050-ConfigMap-strimzi-cluster-operator.yaml
060-Deployment-strimzi-cluster-operator.yaml
----

We will not get into details about the structure of the files, but, for now, it is important to understand that, taken together, they are the complete set of resources required to set up AMQ Streams on an OpenShift cluster.
The files include:

* service account
* cluster roles and and bindings
* a set of CRDs (Custom Resource Definitions) for the objects managed by the AMQ Streams cluster operator
* the cluster operator Deployment

Prior to installing the cluster operator, we will need to configure the namespaces it operates with.
We will do this by modifying the `*RoleBinding*.yaml` files to point to the newly created project `amq-streams`.
You can do this by simply editing all files via `sed`.

----
sed -i 's/namespace: .*/namespace: amq-streams/' install/cluster-operator/*RoleBinding*.yaml
----

Let's take a look at the result in one of the modified files:

----
cat install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml
----

The output should look like this:

----
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: strimzi-cluster-operator
  labels:
    app: strimzi
subjects:
  - kind: ServiceAccount
    name: strimzi-cluster-operator
    namespace: amq-streams
roleRef:
  kind: ClusterRole
  name: strimzi-cluster-operator-namespaced
  apiGroup: rbac.authorization.k8s.io
----

Notice the `amq-streams` value configured for the `subjects.namespace` property.
You can check the other `*RoleBinding*.yaml` files for similar changes.

Now that the configuration files have been set, we can proceed with installing the cluster operator.

=== Installing the cluster operator

Once the configuration files are changed, you can install the cluster operator:

----
oc apply -f install/cluster-operator -n amq-streams
----

Check the status of the deployment:

----
oc get deployments -n amq-streams

NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
strimzi-cluster-operator   1/1     1            1           4m50s
----

For visualizing the result, log into the OpenShift console with the `admin` user.
Navigate to the `amq-streams` project and visualize the current deployments.
You should see the `strimzi-cluster-operator` running.
You have just deployed the cluster operator.

=== Creating an Apache Kafka cluster

It is time to start an Apache Kafka cluster.
We will create now the most basic cluster possible.

----
cat << EOF | oc apply -f -
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: simple-cluster
  namespace: amq-streams
spec:
  kafka:
    replicas: 1
    listeners:
    - name: plain
      port: 9092
      type: internal
      tls: false
    - name: tls
      port: 9093
      type: internal
      tls: true
    config:
      offsets.topic.replication.factor: 1
      transaction.state.log.replication.factor: 1
      transaction.state.log.min.isr: 1
    storage:
      type: ephemeral
  zookeeper:
    replicas: 1
    storage:
      type: ephemeral
  entityOperator:
    topicOperator: {}
    userOperator: {}
EOF
----

Check the status of the deployment:
----
oc get Pods -n amq-streams

NAME                                              READY   STATUS    RESTARTS   AGE
simple-cluster-entity-operator-7cf967b977-tskj8   3/3     Running   0          48s
simple-cluster-kafka-0                            1/1     Running   0          71s
simple-cluster-zookeeper-0                        1/1     Running   0          103s
strimzi-cluster-operator-676c86db94-445lm         1/1     Running   0          16m
----

Again, follow the deployment from the OpenShift console.
You should see three separate deployments:

* `simple-cluster-zookeeper` - a stateful set containing the Zookeeper ensemble
* `simple-cluster-kafka` - a stateful set containing the Kafka cluster
* `simple-cluster-entity-operator` - a deployment containing the entity operator for managing topics and users

=== Testing the deployment

Now, let's quickly test that the deployed Kafka cluster works.
Let's log into one of the cluster pods:

----
$ oc -n amq-streams rsh simple-cluster-kafka-0
----

Next, let's start a producer:

----
$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test-topic
----

Once the console producer is started, enter a few values:

----
> test
> test2
----

(Do not worry if you see the warnings below.
They are part of the interaction and indicate that the topic has not been found and broker will autocreate the `test-topic`.
The message `test` will be properly received by Kafka).

----
>test
[2024-03-17 20:19:45,971] WARN [Producer clientId=console-producer] Error while fetching metadata with correlation id 4 : {test-topic=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient)
>test2
>
----

Now let's open another terminal into the cluster pod in a separate terminal (open another `ssh` terminal into the workstation):

----
$ oc -n amq-streams rsh simple-cluster-kafka-0
----

And let's start a consumer:

----
bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test-topic --from-beginning
----

Once the consumer is started, you should see the previously sent messages in the output.
Reverting to the terminal where we started the console producer and sending any new messages there will result in those messages being displayed in the consumer terminal.

Now let's stop both producer and consumer applications with `CTRL-C` and then exit from the terminal of both containers.

----
exit
----

=== Kafka clusters and Kafka resources

The Kafka resource we just created is a representation of the running Kafka cluster.
You can use it to inspect and modify the current cluster configuration.
For example:

----
oc get -n amq-streams kafka simple-cluster -o yaml
----

Will yield a detailed representation of the resource on the cluster:

----
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"kafka.strimzi.io/v1beta2","kind":"Kafka","metadata":{"annotations":{},"name":"simple-cluster","namespace":"amq-streams"},"spec":{"entityOperator":{"topicOperator":{},"userOperator":{}},"kafka":{"config":{"offsets.topic.replication.factor":1,"transaction.state.log.min.isr":1,"transaction.state.log.replication.factor":1},"listeners":[{"name":"plain","port":9092,"tls":false,"type":"internal"},{"name":"tls","port":9093,"tls":true,"type":"internal"}],"replicas":1,"storage":{"type":"ephemeral"}},"zookeeper":{"replicas":1,"storage":{"type":"ephemeral"}}}}
  creationTimestamp: "2024-03-17T20:16:41Z"
  generation: 1
  name: simple-cluster
  namespace: amq-streams
  resourceVersion: "302978"
  uid: e6a5af1d-3644-482b-a58a-2a74d8ca5851
spec:
  entityOperator:
    topicOperator: {}
    userOperator: {}
  kafka:
    config:
      offsets.topic.replication.factor: 1
      transaction.state.log.min.isr: 1
      transaction.state.log.replication.factor: 1
    listeners:
    - name: plain
      port: 9092
      tls: false
      type: internal
    - name: tls
      port: 9093
      tls: true
      type: internal
    replicas: 1
    storage:
      type: ephemeral
  zookeeper:
    replicas: 1
    storage:
      type: ephemeral
----

Finally, let's delete the Kafka cluster.
We will replace it with a configuration that is more appropriate for real world use cases.

----
oc delete -n amq-streams kafka simple-cluster
----

=== Conclusion

In this workshop module, you have:

* Configured and Installed AMQ Streams
* Deployed a simple Kafka cluster
* Run a producer and consumer to validate the settings
