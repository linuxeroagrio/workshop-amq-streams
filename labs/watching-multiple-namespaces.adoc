== Separating namespace: watching multiple namespaces and delegating administration roles

In the examples we've seen so far, the cluster operator as well as the Kafka cluster, topic and user CRDs were deployed in the same project/namespace.
AMQ Streams allows the cluster operator to monitor a set of separate namespaces, which could be useful for teams that want to manage their own individual clusters.
Each namespace is independent of the other, and resources created in each of them will result in the creation of clusters, topics and users in that specific namespace.

AMQ Streams also allows regular users to take AMQ Streams administration roles, removing the need for cluster administrator permissions for day to day operations.

=== Creating the namespaces

Let's set up two separate projects: `team-1` and `team-2`.

Execute the following commands:

----
oc new-project team-1
----

and

----
oc new-project team-2
----

For this lab, we will still deploy the cluster operator in the `amq-streams` project.
If you've already done so, we will reconfigure it.
So let's revert to the original project `amq-streams`.

----
oc project amq-streams
----

=== Configuring the cluster operator to watch multiple namespaces

First, update the configuration files to reference the target namespace of the cluster operator.
If you've already done so in the first lab, skip this step.

----
sed -i 's/namespace: .*/namespace: amq-streams/' install/cluster-operator/*RoleBinding*.yaml
----

Next, alter the file `install/cluster-operator/060-Deployment-strimzi-cluster-operator.yaml` to point to the monitored projects. Ensure that the projects `amq-streams` `team-1` and `team-2` are in the `STRIMZI_NAMESPACE` variable.

----
vi install/cluster-operator/060-Deployment-strimzi-cluster-operator.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: strimzi-cluster-operator
spec:
  template:
    spec:
      containers:
        - name: strimzi-cluster-operator
          env:
            - name: STRIMZI_NAMESPACE
              value: amq-streams,team-1,team-2
----

Next, install `RoleBinding`s for each of the two monitored namespaces:

----
oc apply -f install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml -n team-1
oc apply -f install/cluster-operator/023-RoleBinding-strimzi-cluster-operator.yaml -n team-1
oc apply -f install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml -n team-1
----

and

----
oc apply -f install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml -n team-2
oc apply -f install/cluster-operator/023-RoleBinding-strimzi-cluster-operator.yaml -n team-2
oc apply -f install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml -n team-2
----

Finally, install - or re-install and reconfigure the `cluster-operator`.

----
oc apply -f install/cluster-operator -n amq-streams
----

Now we can deploy clusters, topics and users in each of these namespaces.

Let's deploy
----
cat << EOF | oc apply -f -
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: production-ready
  namespace: team-1
spec:
  kafka:
    replicas: 3
    listeners:
    - name: plain
      port: 9092
      type: internal
      tls: false
    - name: tls
      port: 9093
      type: internal
      tls: true
    config:
      offsets.topic.replication.factor: 3
      transaction.state.log.replication.factor: 3
      transaction.state.log.min.isr: 2
    storage:
      type: persistent-claim
      size: 3Gi
      deleteClaim: false
  zookeeper:
    replicas: 3
    storage:
      type: persistent-claim
      size: 1Gi
      deleteClaim: false
  entityOperator:
    topicOperator: {}
    userOperator: {}
EOF
----

Observe the pods and pvc in the team-1 namespace

----
oc get pods,pvc -n team-1

NAME                                                    READY   STATUS    RESTARTS   AGE
pod/production-ready-entity-operator-6667995c9c-lvqkk   3/3     Running   0          48s
pod/production-ready-kafka-0                            1/1     Running   0          78s
pod/production-ready-kafka-1                            1/1     Running   0          78s
pod/production-ready-kafka-2                            1/1     Running   0          78s
pod/production-ready-zookeeper-0                        1/1     Running   0          105s
pod/production-ready-zookeeper-1                        1/1     Running   0          105s
pod/production-ready-zookeeper-2                        1/1     Running   0          105s

NAME                                                      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                           AGE
persistentvolumeclaim/data-production-ready-kafka-0       Bound    pvc-426c58d1-33a1-4728-b612-07f8494f9ddb   3Gi        RWO            ocs-external-storagecluster-ceph-rbd   79s
persistentvolumeclaim/data-production-ready-kafka-1       Bound    pvc-9ce96bd4-c3c3-4e9b-baa2-e416834f32d2   3Gi        RWO            ocs-external-storagecluster-ceph-rbd   79s
persistentvolumeclaim/data-production-ready-kafka-2       Bound    pvc-881f1330-2cba-4fc8-ac35-a0eb8a873c6c   3Gi        RWO            ocs-external-storagecluster-ceph-rbd   79s
persistentvolumeclaim/data-production-ready-zookeeper-0   Bound    pvc-79738a3d-2559-4f3c-9970-cf64df5ced78   1Gi        RWO            ocs-external-storagecluster-ceph-rbd   106s
persistentvolumeclaim/data-production-ready-zookeeper-1   Bound    pvc-eaa5d5a9-4c9f-40c1-8df8-6731f0977b57   1Gi        RWO            ocs-external-storagecluster-ceph-rbd   106s
persistentvolumeclaim/data-production-ready-zookeeper-2   Bound    pvc-161ac47c-8c6c-4859-b68f-6c18a34b8a28   1Gi        RWO            ocs-external-storagecluster-ceph-rbd   106s
----

Let's see that the cluster works.

Reconfigure the `timer-producer` and `log-consumer` applications to use the new cluster.

----
cat << EOF | oc replace -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: kafka-workshop
  name: timer-producer
  namespace: amq-streams
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kafka-workshop
      name: timer-producer
  template:
    metadata:
      labels:
        app: kafka-workshop
        name: timer-producer
    spec:
      containers:
      - image: docker.io/mbogoevici/timer-producer:latest
        name: timer-producer
        env:
        - name: CAMEL_COMPONENT_KAFKA_CONFIGURATION_BROKERS
          value: "production-ready-kafka-bootstrap.team-1.svc:9092"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: kafka-workshop
  name: log-consumer
  namespace: amq-streams
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kafka-workshop
      name: log-consumer
  template:
    metadata:
      labels:
        app: kafka-workshop
        name: log-consumer
    spec:
      containers:
      - image: docker.io/mbogoevici/log-consumer:latest
        name: log-consumer
        env:
        - name: CAMEL_COMPONENT_KAFKA_CONFIGURATION_BROKERS
          value: "production-ready-kafka-bootstrap.team-1.svc:9092"
EOF
----

Once the applications have restarted, navigate to the logs and you should see the messages flowing again.

----
oc get pods -l app=kafka-workshop -n amq-streams

CONSUMER_POD_NAME=$(oc get pods -l app=kafka-workshop -l name=log-consumer -n amq-streams -o jsonpath='{.items[0].metadata.name}')

oc logs -f pod/$CONSUMER_POD_NAME  -c log-consumer --tail=-1
----

The applications deployed in the `amq-streams` namespace will interact with a Kafka cluster configured in the `team-1` namespace.

=== Strimzi Administrators

So far, we have used a cluster administrator to set up and manage Kafka clusters and topics.
AMQ Streams allows the assignment of administrative permissions to regular user for day-to-day operations, once the cluster operator has been installed.

==== Creating OpenShift users

OpenShift allows different strategies for creating users.
In this lab we will create simple users authenticated against the OpenShift configuration files.

First, let's add `dev-team-1` and `dev-team-2` users:
----
oc extract secret/htpasswd -n openshift-config --confirm --to=.

htpasswd -bB htpasswd dev-team-1 openshift
htpasswd -bB htpasswd dev-team-2 openshift

oc set data secret/htpasswd --from-file=htpasswd=htpasswd -n openshift-config

watch oc get pods -n openshift-authentication
----

Assign the two users to the previously created projects:

----
oc adm policy add-role-to-user admin dev-team-1 -n team-1
oc adm policy add-role-to-user admin dev-team-2 -n team-2
----

[NOTE]
Ignore the warning related to the existing of the users. The users will be created when they do login.

Log into one of the users and change current project:

----
APISERVER_URL=$(oc whoami --show-server)

oc login -u dev-team-2 -p openshift --server ${APISERVER_URL} --insecure-skip-tls-verify=true
----

Change the cluster configuration:

----
cat << EOF | oc apply -f -
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: production-ready
  namespace: team-2
spec:
  kafka:
    replicas: 3
    listeners:
    - name: plain
      port: 9092
      type: internal
      tls: false
    - name: tls
      port: 9093
      type: internal
      tls: true
    config:
      offsets.topic.replication.factor: 3
      transaction.state.log.replication.factor: 3
      transaction.state.log.min.isr: 2
    storage:
      type: persistent-claim
      size: 3Gi
      deleteClaim: false
  zookeeper:
    replicas: 3
    storage:
      type: persistent-claim
      size: 1Gi
      deleteClaim: false
  entityOperator:
    topicOperator: {}
    userOperator: {}
EOF
----

You should see the operation failing with an error along the lines of `Error from server (Forbidden):`.
Your user does not have permission to update the custom resources.

To correct that, we will first create a `StrimziAdmin` cluster role that we can assign to users.
Log in as `admin` and apply the roles.

----
oc login -u system:admin
oc create -f install/strimzi-admin
----

Assign the cluster role to the newly created users.

----
oc create clusterrolebinding strimzi-admin --clusterrole=strimzi-admin --user=dev-team-1 --user=dev-team-2
----

Now log in again try to repeat the operation.

----
APISERVER_URL=$(oc whoami --show-server)

oc login -u dev-team-2 -p openshift --server ${APISERVER_URL} --insecure-skip-tls-verify=true

cat << EOF | oc apply -f -
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: production-ready
  namespace: team-2
spec:
  kafka:
    replicas: 3
    listeners:
    - name: plain
      port: 9092
      type: internal
      tls: false
    - name: tls
      port: 9093
      type: internal
      tls: true
    config:
      offsets.topic.replication.factor: 3
      transaction.state.log.replication.factor: 3
      transaction.state.log.min.isr: 2
    storage:
      type: persistent-claim
      size: 3Gi
      deleteClaim: false
  zookeeper:
    replicas: 3
    storage:
      type: persistent-claim
      size: 1Gi
      deleteClaim: false
  entityOperator:
    topicOperator: {}
    userOperator: {}
EOF
----

You should see the new cluster being created.

----
oc get pods,pvc -n team-2

NAME                                                    READY   STATUS    RESTARTS   AGE
pod/production-ready-entity-operator-54fcc46958-rbzh9   3/3     Running   0          44s
pod/production-ready-kafka-0                            1/1     Running   0          74s
pod/production-ready-kafka-1                            1/1     Running   0          74s
pod/production-ready-kafka-2                            1/1     Running   0          74s
pod/production-ready-zookeeper-0                        1/1     Running   0          103s
pod/production-ready-zookeeper-1                        1/1     Running   0          103s
pod/production-ready-zookeeper-2                        1/1     Running   0          103s

NAME                                                      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                           AGE
persistentvolumeclaim/data-production-ready-kafka-0       Bound    pvc-1032ed29-3713-4686-ade2-be05de15482f   3Gi        RWO            ocs-external-storagecluster-ceph-rbd   74s
persistentvolumeclaim/data-production-ready-kafka-1       Bound    pvc-dcb72598-873a-4285-8763-3dda0f1ba463   3Gi        RWO            ocs-external-storagecluster-ceph-rbd   74s
persistentvolumeclaim/data-production-ready-kafka-2       Bound    pvc-c8b3e657-4308-4982-a8ea-c72d1ad32e7f   3Gi        RWO            ocs-external-storagecluster-ceph-rbd   74s
persistentvolumeclaim/data-production-ready-zookeeper-0   Bound    pvc-05efacb5-12dd-4cc7-bf16-d3866339cd32   1Gi        RWO            ocs-external-storagecluster-ceph-rbd   103s
persistentvolumeclaim/data-production-ready-zookeeper-1   Bound    pvc-5785897b-93b2-4b94-95cf-a53e1d064051   1Gi        RWO            ocs-external-storagecluster-ceph-rbd   103s
persistentvolumeclaim/data-production-ready-zookeeper-2   Bound    pvc-b6700dd8-b092-4b04-970b-fcfca3e63e43   1Gi        RWO            ocs-external-storagecluster-ceph-rbd   103s
----