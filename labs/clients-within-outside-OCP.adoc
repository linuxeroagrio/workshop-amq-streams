== Connecting from outside OCP

As you have seen, a Kafka cluster deployed in OpenShift can be used by other applications deployed in the same OpenShift instance.
This is the primary use case.

=== Configuring external routes

In certain scenarios, a Kafka cluster deployed in OpenShift may need to be accessed from outside the cluster.
Let's see how that can be enabled.

First, we need to reconfigure the cluster with an `external` listener.

----
cat << EOF | oc apply -f -
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: production-ready
  namespace: amq-streams
spec:
  kafka:
    replicas: 3
    listeners:
    - name: plain
      port: 9092
      type: internal
      tls: false
    - name: tls
      port: 9093
      type: internal
      tls: true
    - name: external
      port: 9094
      type: route
      tls: true
    config:
      offsets.topic.replication.factor: 3
      transaction.state.log.replication.factor: 3
      transaction.state.log.min.isr: 2
    storage:
      type: persistent-claim
      size: 3Gi
      deleteClaim: false
  zookeeper:
    replicas: 3
    storage:
      type: persistent-claim
      size: 1Gi
      deleteClaim: false
  entityOperator:
    topicOperator: {}
    userOperator: {}
EOF
----

You can see that the Cluster Operator is starting a rolling update on the Kafka cluster, restarting each broker one by one for updating their configuration in order to expose the cluster outside of OpenShift.
Now you can inspect the existing services.
Notice that each of the brokers in the cluster has a route and a new bootstrap service has been created for external connections.

----
oc get routes -n amq-streams
----

=== Connecting external clients

For interacting with the broker, external clients must use TLS.
First, we need to extract the certificate of the server.
----
oc extract -n amq-streams secret/production-ready-cluster-ca-cert --keys=ca.crt --to=- > certificate.crt
----

Then, we need to install it into a Java keystore.

----
sudo yum -y install java-1.8.0-openjdk-devel java-1.8.0-openjdk-headless java-1.8.0-openjdk

keytool -import -trustcacerts -alias root -file certificate.crt -keystore keystore.jks -storepass password -noprompt
----

We can now run our producer and consumer applications by using this certificate.

Let's download the two JARs.

----
wget -O log-consumer.jar https://github.com/RedHatWorkshops/workshop-amq-streams/blob/master/bin/log-consumer.jar?raw=true
wget -O timer-producer.jar https://github.com/RedHatWorkshops/workshop-amq-streams/blob/master/bin/timer-producer.jar?raw=true
----

Let's launch the two applications with new configuration settings (replace <GUID> with your workstation GUID):

----
java -jar log-consumer.jar \
      --camel.component.kafka.configuration.brokers=production-ready-kafka-bootstrap-amq-streams.apps.cluster-<GUID>.dynamic.redhatworkshops.io:443 \
      --camel.component.kafka.configuration.security-protocol=SSL \
      --camel.component.kafka.configuration.ssl-truststore-location=keystore.jks \
      --camel.component.kafka.configuration.ssl-truststore-password=password
----

Open a new terminal and run the producer application

----
cd kafka

java -jar timer-producer.jar \
      --camel.component.kafka.configuration.brokers=production-ready-kafka-bootstrap-amq-streams.apps.cluster-<GUID>.dynamic.redhatworkshops.io:443 \
      --camel.component.kafka.configuration.security-protocol=SSL \
      --camel.component.kafka.configuration.ssl-truststore-location=keystore.jks \
      --camel.component.kafka.configuration.ssl-truststore-password=password --server.port=0
----

After observing how the two clients are exchaning messages from outside the OpenShift cluster, exit both the applications via `CTRL-C`.