== Security for clusters and topics

In this module we will show you how to secure a cluster and how to create and manage users.

=== Securing listeners

The first step for securing a Kafka cluster is securing its listeners.
You can add security options to each of the configured listeners.
For example, let us change the cluster definition for the plain listener.

----
cat << EOF | oc replace -f -
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: production-ready
  namespace: amq-streams
spec:
  kafka:
    replicas: 3
    listeners:
    - name: plain
      port: 9092
      type: internal
      tls: false
      authentication:
        type: scram-sha-512
    authorization:
      type: simple
    config:
      offsets.topic.replication.factor: 3
      transaction.state.log.replication.factor: 3
      transaction.state.log.min.isr: 2
    storage:
      type: persistent-claim
      size: 3Gi
      deleteClaim: false
  zookeeper:
    replicas: 3
    storage:
      type: persistent-claim
      size: 1Gi
      deleteClaim: false
  entityOperator:
    topicOperator: {}
    userOperator: {}
EOF
----

Watch for the changes in the stateful set.
Once all pods have been restarted, you can proceed.
The `plain` listener is now configured to use the `SCRAM-SHA-512` challenge mechanism for connecting clients.

=== Creating users and ACLs

Now that we have configured the broker to be secured, we need to create users so that our clients can connect.
Users are managed through `KafkaUser` resources, which also manage the user authorization.
Let's create our first user.

----
cat << EOF | oc apply -f -
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaUser
metadata:
  name: secure-topic-reader
  namespace: amq-streams
  labels:
    strimzi.io/cluster: production-ready
spec:
  authentication:
    type: scram-sha-512
  authorization:
    type: simple
    acls:
      # Example consumer Acls for topic lines using consumer group secure-group
      - resource:
          type: topic
          name: lines
          patternType: literal
        operations:
          - Read
          - Describe
        host: "*"
      # Example ACL rules for producing to topic my-topic
      - resource:
          type: group
          name: secure-group
          patternType: literal
        operations:
          - Read
        host: "*"
EOF
----

The newly created user can read the metadata of topic `lines` and consume (read) from it with the consumer group `secure-group`.

But now we need a user that can produce data to `lines`!
Let's create a new resource:

----
cat << EOF | oc apply -f -
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaUser
metadata:
  name: secure-topic-writer
  namespace: amq-streams
  labels:
    strimzi.io/cluster: production-ready
spec:
  authentication:
    type: scram-sha-512
  authorization:
    type: simple
    acls:
      # Example Producer Acls for topic secure-topic
      - resource:
          type: topic
          name: lines
          patternType: literal
        operations:
          - Write
        host: "*"
EOF
----

Go to `Secrets` and observe that new secret named `secure-topic-reader` and `secure-topic-writer` have been created.
Both secrets have a field named `password`.

----
oc get secrets secure-topic-reader secure-topic-writer -n amq-streams

oc extract -n amq-streams secret/secure-topic-reader --confirm --to=- --keys=password

oc extract -n amq-streams secret/secure-topic-writer --confirm --to=- --keys=password
----

Now let's redeploy our running applications by running on the OpenShift cluster.

----
cat << EOF | oc apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: kafka-workshop
  name: log-consumer
  namespace: amq-streams
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kafka-workshop
      name: log-consumer
  template:
    metadata:
      labels:
        app: kafka-workshop
        name: log-consumer
    spec:
      containers:
      - image: docker.io/mbogoevici/log-consumer:latest
        name: log-consumer
        env:
        - name: CAMEL_COMPONENT_KAFKA_CONFIGURATION_BROKERS
          value: "production-ready-kafka-bootstrap.amq-streams.svc:9092"
        - name: CAMEL_COMPONENT_KAFKA_CONFIGURATION_GROUP_ID
          value: test-group
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: kafka-workshop
  name: timer-producer
  namespace: amq-streams
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kafka-workshop
      name: timer-producer
  template:
    metadata:
      labels:
        app: kafka-workshop
        name: timer-producer
    spec:
      containers:
      - image: docker.io/mbogoevici/timer-producer:latest
        name: timer-producer
        env:
        - name: CAMEL_COMPONENT_KAFKA_CONFIGURATION_BROKERS
          value: "production-ready-kafka-bootstrap.amq-streams.svc:9092"
EOF
----

Looking at the logs, we see a lot of errors - the clients cannot connect anymore.

We need to reconfigure the running apps:
----
cat << "EOF" | oc replace -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: kafka-workshop
  name: timer-producer
  namespace: amq-streams
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kafka-workshop
      name: timer-producer
  template:
    metadata:
      labels:
        app: kafka-workshop
        name: timer-producer
    spec:
      containers:
      - image: docker.io/mbogoevici/timer-producer:latest
        name: timer-producer
        env:
        - name: CAMEL_COMPONENT_KAFKA_CONFIGURATION_BROKERS
          value: "production-ready-kafka-bootstrap.amq-streams.svc:9092"
        - name: CAMEL_COMPONENT_KAFKA_CONFIGURATION_SASL_MECHANISM
          value: SCRAM-SHA-512
        - name: CAMEL_COMPONENT_KAFKA_CONFIGURATION_SECURITY_PROTOCOL
          value: SASL_PLAINTEXT
        - name: KAFKA_USER
          value: secure-topic-writer
        - name: KAFKA_PASSWORD
          valueFrom:
            secretKeyRef:
              key: password
              name: secure-topic-writer
        - name: CAMEL_COMPONENT_KAFKA_CONFIGURATION_SASL_JAAS_CONFIG
          value: org.apache.kafka.common.security.scram.ScramLoginModule required username='${KAFKA_USER}' password='${KAFKA_PASSWORD}';
EOF
----

We need to secure the `log-consumer` application as well:

----
cat << "EOF" | oc replace -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: kafka-workshop
  name: log-consumer
  namespace: amq-streams
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kafka-workshop
      name: log-consumer
  template:
    metadata:
      labels:
        app: kafka-workshop
        name: log-consumer
    spec:
      containers:
      - image: docker.io/mbogoevici/log-consumer:latest
        name: log-consumer
        env:
        - name: CAMEL_COMPONENT_KAFKA_CONFIGURATION_BROKERS
          value: "production-ready-kafka-bootstrap.amq-streams.svc:9092"
        - name: CAMEL_COMPONENT_KAFKA_CONFIGURATION_GROUP_ID
          value: secure-group
        - name: CAMEL_COMPONENT_KAFKA_CONFIGURATION_SASL_JAAS_CONFIG
          value: org.apache.kafka.common.security.scram.ScramLoginModule required username='${KAFKA_USER}' password='${KAFKA_PASSWORD}';
        - name: CAMEL_COMPONENT_KAFKA_CONFIGURATION_SASL_MECHANISM
          value: SCRAM-SHA-512
        - name: CAMEL_COMPONENT_KAFKA_CONFIGURATION_SECURITY_PROTOCOL
          value: SASL_PLAINTEXT
        - name: KAFKA_USER
          value: secure-topic-reader
        - name: KAFKA_PASSWORD
          valueFrom:
            secretKeyRef:
              key: password
              name: secure-topic-reader
EOF
----

Inspect the log of `log-consumer` again.
You should see the messages being exchanged.

----
oc get pods -l app=kafka-workshop -n amq-streams

CONSUMER_POD_NAME=$(oc get pods -l app=kafka-workshop -l name=log-consumer -n amq-streams -o jsonpath='{.items[0].metadata.name}')

oc logs -f pod/$CONSUMER_POD_NAME  -c log-consumer --tail=-1
----